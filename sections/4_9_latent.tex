\section{Latent Variable Models}
\thispagestyle{plain}

Latent variables are unobserved variables used to explain the observed data.
Models including latent variables can have many applications.

\begin{itemize}
    \item \textbf{Finding latent sources}: From measurements, we want to find the underlying, latent signals, so rather
    than finding a model relating multiple observed variables, we want to find
    underlying unobserved variables linked to our observation. For instance
    \begin{itemize}
        \item we have measured neural activity by $100$ electrodes on the scalp and want to find the underlying neural signals
        \item we have recorded multiple stock prices at many points in time and want to find the underlying factors that drive the stock prices
        \item we did multiple tests on $N$ students and want to find out about underlying factors such as drive or diligence
    \end{itemize}
    \item \textbf{Making a model more flexible}: Consider for instance we have observed data and 
    assume there is also a latent variable giving the class of the data points, which is unobserved.
    In this setting we might be interested in the discriminative model, so the posterior of
    the latent variable given the observed data (i.e. cluster assignment in the example). Or we might want to find parameters of a model
    containing latent variables.
    \item \textbf{Generative modeling}: Based on a latent space representation with a simpler distribution,
    from which we can easily sample, we can generate new data, e.g. in image generation.
\end{itemize}

Regarding finding latent sources, we will discuss

\begin{itemize}
    \item \textbf{Independent Component Analysis (ICA)}: From mixed signals we want to find underlying non-Gaussian, independent sources, e.g. 
    to separate the sound of different instruments from the recordings of multiple microphones.
    \item \textbf{Factor Analysis}: We want to find underlying latent variables (model parameters in a Bayesian setting are latent variables) that explain the common variance in multiple observed variables where
    individual variance is also present, e.g. for test scores in different tests.
\end{itemize}

Regarding models with latent variables to make them more expressive / to
get a generative model, we will discuss

\begin{itemize}
    \item \textbf{Expectation-Maximization:} Essentially we want to do maximum likelihood but for a model with latent variables. We will apply this to Gaussian Mixture models for clustering.
    \item \textbf{Variational Inference:} We want to approximate the posterior of the latent variables given the observed data, so we can do statistical inference in the latent space.
    \item \textbf{Variational Autoencoders:} We want to find a latent space representation of the data with a kind of probabalistic autoencoder,
          so that from samples in latent space we can generate new data in the more complex data space (e.g. DALLE for image generation).
\end{itemize}

\subsection{The Cocktail Party Problem - Blind Source Separation}
Consider three instruments playing at the center of the room, one plays a sine signal
\begin{equation}
    \{S_{i1}\}_{i=1}^N = \{ \sin 2t_i \}_{i=1}^N, \quad t = 0.000:0.004:8.000
\end{equation}
another a square signal
\begin{equation}
    \{S_{i2}\}_{i=1}^N = \{ \text{square}(2t_i) \}_{i=1}^N, \quad t = 0.000:0.004:8.000
\end{equation}
and the third a sawtooth signal
\begin{equation}
    \{S_{i2}\}_{i=1}^N = \{ \text{sawtooth}(2t_i) \}_{i=1}^N, \quad t = 0.000:0.004:8.000
\end{equation}
We measure this signal by three microphones at different positions in the room. They measure different
linearly mixed signals
\begin{equation}
    \begin{aligned}
        X_{i1} &= S_{i1} + S_{i1} + S_{i1} + \epsilon_{i1} \\
        X_{i2} &= 0.5 S_{i2} + 2 S_{i2} + S_{i2} + \epsilon_{i2} \\
        X_{i3} &= 1.5 S_{i3} + S_{i3} + 2 S_{i3} + \epsilon_{i3}
    \end{aligned}
\end{equation}
where the $\epsilon_{i1}$ are the noise terms and we have inserted an arbitrary mixture.

\note{We assume the same mixture at all times, time information is not used. We just consider $N$ samples of the signals. So our transformation
is really a matrix transformation in $p \times p$ space (here $p = 3$).}

In matrix form, we have

\begin{equation}
    \vec{X} = \begin{pmatrix} X_1 \\ X_2 \\ X_3 \end{pmatrix} = \mat{\Gamma} \vec{S} + \vec{\epsilon}
\end{equation}

holding for all $N$ samples. For all samples we can write

\begin{equation}
    \mat{X} = \mat{S} \mat{\Gamma} + \mat{E}, \quad \mat{X}, \mat{S} \in \mathbb{R}^{N \times p}, \quad \mat{\Gamma} \in \mathbb{R}^{p \times p}
\end{equation}

here with $p = 3$. The original signals of the instruments and the microphone recordings
are shown in figure \ref{fig:cocktail_party}.

\begin{figure}[H]
    \centering
    \includesvg[width=0.8\textwidth]{figures/cocktail_party.svg}
    \caption{Mixture and separation of signal. Note the space where we operate is really $p$-dimensional space (here $p=3$),
    so the number of original signals and number of separations. \textbf{The order / time data is not used.} We might try to
    separate the signals by doing PCA in this $p$-dimensional space (leading to $p$ separated signals).}
    \label{fig:cocktail_party}
\end{figure}

\bluebox{\textbf{Aim:} Given $p$ perspectives ($p$ microphones) on a mixed signal, we want to find $q$ (generally $q \leq p$) original \textbf{latent} (not directly observed)
source signals.}

\subsection{Independent Component Analysis (ICA)}

\subsubsection{Intuition}
\bluebox{From mixes we want to find original not directly observable (latent) signals.}
Consider two uniformly distributed source signals $S_1, S_2$ and the mixes
\begin{equation}
    \begin{aligned}
        X_1 &= S_1 + \frac{1}{2} S_2 \\
        X_2 &= S_1 - \frac{1}{2} 3S_2
    \end{aligned}
\end{equation}
This is illustrated in figure \ref{fig:ica_illu}.

\begin{figure}[!htb]
    \centering
    \includesvg[width=0.8\textwidth]{figures/ica_illu2.svg}
    \caption{Illustration of ICA.}
    \label{fig:ica_illu}
\end{figure}

\textbf{Rough idea:} Loosely speaking when we add two signals, the mixture becomes more Gaussian,
so we might try to find sources by finding a transformation that makes the result as non-Gaussian as possible,
or relatedly maximize their independence.

\note{We first whiten the data, i.e. remove any correlations, the different channels in $\vec{X}$ are forced to
be uncorrelated. Here the intuition is that after whitening, ICA only needs to rotate the data-matrix for
a high independence of the sources.}

\subsubsection{Formalization and aim}
\redbox{Different from before let $\mat{X} \in \mathbb{R}^{p \times N}$ be the matrix with $p$ features and $N$ samples,
as it makes the notation here a bit nicer. We assume $\mat{X}$ to have centered and normalized features.}

We assume the model
\begin{equation}
    \begin{gathered}
        \underset{p\times N}{\mat{X}} = \underset{p\times q}{\mat{\Gamma}} \, \underset{q\times N}{\mat{S}}, \quad q \leq p \\
        \quad \text{latent sources } \mat{S}, \text{linear mixing matrix } \mat{\Gamma}, \quad \text{observed signals } \mat{X}
    \end{gathered}
\end{equation}
so without a noise term\footnote{There is also noisy ICA.}.

\textbf{In independent component analysis (ICA), the $S_i$ are assumed to be statistically independent (not just
uncorrelated) and non-gaussian.}\footnote{Independent Gaussian components can only be determined up to a rotation.}

\bluebox{\textbf{Aim:} Find the mixing matrix $\mat{\Gamma}$ and the latent sources $\mat{S}$ from the observed signals $\mat{X}$,
such that the sources are as independent as possible, \enquote{$p(\vec{S}) = p(S_1) \cdot p(S_2) \cdot \dots \cdot p(S_q)$}.}

\subsubsection{Pre-step: Whitening of the observed data}

Consider the singular-value decomposition $\mat{X} = \mat{U} \mat{D} \mat{V}^T$ ($\mat{U} \in \mathbb{R}^{p \times p}$).
Since the rows of $\mat{U}$ and $\mat{V}$ are orthonormal vectors, $\mat{U}\mat{V}^T$ will be white ($(\mat{U}\mat{V}^T)(\mat{U}\mat{V}^T)^T 
= \mat{U}(\mat{V}^T\mat{V})\mat{U}^T = \mat{1}$). Which
we use as our $\mat{X}$ henceforth.

\greenbox{Since $\mat{X}$ is whitened to $\Cov(\vec{X}) = \mat{1}$ (so for centered features $\Cov(\vec{X}) = \frac{1}{N} \mat{X} \mat{X}^T = \mat{1}$, so $\mat{X}$
orthogonal) and we also request covariance of the sources to be $\Cov(\vec{S}) = \mat{1}$ (a weaker condition than independence), $\mat{\Gamma}$ must
be orthogonal, so $\mat{\Gamma}^{-1} = \mat{\Gamma}^T$.}

\bluebox{\textbf{Reformulated aim:} Find an orthogonal $\mat{\Gamma}$ such that the components of the vector random
variable $\vec{S} = \mat{\Gamma}^T \vec{X}$ are independent (and non-Gaussian).}

\yellowbox{\textbf{Why does requesting no correlation not give unique sources?:} Assume we would only 
request that the sources are uncorrelated, so $\Cov(\vec{S}) = \mat{1}$. As discussed, no correlation
can be obtained by whitening. However, consider 
\begin{equation}
    \vec{X} = \mat{\Gamma} \vec{S} = \mat{\Gamma} \mat{R} \mat{R}^T \vec{S} = (\mat{\Gamma} \mat{R}) (\mat{R}^T \vec{S}) = \mat{\Gamma^*} \vec{S^*}
\end{equation}
then $\Cov(\vec{S^*}) = \mat{R} \Cov(\vec{S}) \mat{R}^T = \mat{R} \mat{1} \mat{R}^T = \mat{1}$, so there
are many such compositions and it is impossible to recover unique underlying sources. Consider
figure \ref{fig:ica_illu}. \textbf{All rotations of the whitened signal are also 
uncorrelated (white) but only one, the ICA rotation, yields independent sources.}}

\yellowbox{\textbf{Why do we exclude Gaussian sources?} Any Gaussian independent components can only be determined
up to a rotation, so we would have the same non-uniqueness problem as with uncorrelated sources.}

\subsubsection{Minimizing dependence of the sources}
Dependence of random variables can be measured by the mutual information (with $p(S_i)$ the marginal
densities of the source-components)
\begin{equation}
    \begin{aligned}
        \operatorname{MI}(\vec{S}) &= \explain{D_{KL}\left( p(\vec{S}) \, || \, \prod_{i=1}^q p(S_i) \right)}{Kullback-Leibler divergence aka Relative Entropy} \\
        &= \int p(\vec{S}) \log \frac{p(\vec{S})}{\prod_{i=1}^q p(S_i)} \, d\vec{S} \\
        &= -\int p(\vec{S}) \sum_{i=1}^{q} \log p(S_i) \, d\vec{S} + \int p(\vec{S}) \log p(\vec{S}) \, d\vec{S} \\
        &= -\sum_{i=1}^{q} \int p(\vec{S}) \log p(S_i) \, d\vec{S} + H(\vec{S}) \\
        &= -\sum_{i=1}^{q} \int \log p(S_i) \underbrace{\int p(\vec{S}) \, dS_1 \dots dS_{i-1} dS_{i+1} \dots dS_q}_{=\, p(S_i)} \, dS_i + H(\vec{S}) \\
        &= \sum_{i=1}^{q} H(S_i) - H(\vec{S})
    \end{aligned}
\end{equation}
where we used the expression for the entropy
\begin{equation}
    H(\vec{S}) = - \int p(\vec{S}) \log p(\vec{S}) \, d\vec{S}
\end{equation}

Assuming $\Cov(\vec{S}) = \mat{1}$ without loss of generality (can be enforced by choice of $\mat{\Gamma}$), and
with $\Cov(\vec{X}) = \mat{1}$, we have $\vec{S} = \mat{\Gamma}^T \vec{X} \in \mathbb{R}^{q}$, from which
one can show $H(\vec{S}) = H(\vec{X}) + \log |\det \mat{\Gamma}| = H(\vec{X})$ (\textcolor{blue1}{an orthogonal
transformation does not change a distribution's entropy}), so

\begin{equation}
    \operatorname{MI}(\vec{S}) = \sum_{i=1}^{q} H(S_i) - H(\vec{X})
\end{equation}

so

\begin{equation}
    \mat{\hat{\Gamma}} = \argmin_{\mat{\Gamma}} \sum_{i=1}^{q} H((\mat{\Gamma}^T \vec{X})_i) - \underbrace{H(\vec{X})}_{=\text{const.}} = \argmin_{\mat{\Gamma}} \sum_{i=1}^{q} H((\mat{\Gamma}^T \vec{X})_i)
\end{equation}

\note{This is equivalent to minimizing the sum of the entropies of the separate components of $\vec{S}$, which is
equivalent to maximizing their non-Gaussianity (e.g. Wasserschein distance to a Gaussian).}

\problem{We want to calculate 
\begin{equation}
    H(S_i) = H((\mat{\Gamma}^T \vec{X})_i)
\end{equation}
but given the $\mat{\Gamma}$ we only have $N$ samples $S_{1}^{(1)}, S_{i}^{(2)}, \dots, S_{i}^{(N)}$ of the $i$-th source, not a probability
distribution $p(S_i)$.
}
\idea{There are direct empirical estimates of the entropy (alternatively estimate the density), e.g.
\begin{equation}
    \hat{H}_\text{RADICAL}(S_i) = \frac{1}{N-m} \sum_{k=1}^{N-m} \log \left( \frac{N+1}{m} \left( S_{i}^{(k+m)} - S_{i}^{(k)} \right) \right)
\end{equation}
with a small integer $m$ (a parameter) (e.g. $m=1$). This is called ICA with spacings estimates of entropy, see \cite{miller03}.
}

\note{ICA cannot recover
\begin{itemize}
    \item the actual number of source signals
    \item a uniquely correct ordering of the source signals
    \item the proper scaling (including sign) of the source signals
    \item ICA can only extract sources that were mixed linearly
\end{itemize}
}

\note{While it might look like it, ICA is not an inverse problem in the classical sense
because there is no forward model to invert.}

\subsection{Factor Analysis}
\subsubsection{Going deeper than relating observed data - in search of the unobserved}
Consider we have observed temperatures $y_i$ and climate factors $x_{ij}$, which we relate
by a linear model
\begin{equation}
    y_i = \sum_{j=1}^{p} \beta_j x_{ij} + \epsilon_i
\end{equation}
in which the $\beta_j$ can be determined by linear regression as usual.
A step deeper we can ask ourselves what well separated unknown sources
$\vec{S}$ can have given rise to $\vec{x}$ and $y$, e.g. industrial policies, etc.

\subsubsection{Setting of Factor Analysis}
Consider we have observed $p$ features $\vec{x} \in \mathbb{R}^{p}$.
\idea{What if there are common latent sources $S_1,\dots, S_q$ ($\vec{S} \in \mathbb{R}^{q},q < p$) of the
variation amongst the features? What if the variation in many observed features only stems from variation
in a few underlying sources that are just combined in different ways?}
For instance the variation in a set stock prices (e.g. all from the DAX) might be driven by a few common latent factors
like central bank policies etc. which we want to find. Or from a set of tests in school, we might want
to find common factors like drive, diligence, etc.

Consider $N$ \textit{individuals} (e.g. students) and $p$ \textit{records} (e.g. test scores in different tests).

\begin{equation}
    \begin{gathered}
        \underset{p\times 1}{\vec{x}_i} = \underset{p \times 1}{\vec{\mu}} + \underset{p \times q}{\mat{\Gamma}} \, \underset{q \times 1}{\vec{S}_i} + \underset{p \times 1}{\vec{\epsilon}_i}, \quad i = 1,\dots,N \\
        \text{observations } \vec{x}_i, \quad \text{latent sources } \vec{S}_i, \quad \text{loading matrix } \mat{\Gamma}\\
        \quad \text{observation mean } \vec{\mu} \approxeq \frac{1}{N} \sum_{i=1}^{N} \vec{x}_i, \quad \text{noise } \vec{\epsilon}_i
    \end{gathered}
\end{equation}

where in the example of the students
\begin{itemize}
    \item $\vec{x}_i$ are the test scores of student $i$ in the different tests
    \item $\vec{S}_i$ are the values of the latent variables for student $i$, e.g. interpreted as
    student $i$'s diligence, drive, etc., the few underlying factors that mainly drive variations in the test scores
    \item $\vec{\epsilon}_i$ is the noise term, the part of the test scores that is not explained by the latent variables
    \item $\mat{\Gamma}$ is the loading matrix, which tells us how the latent variables are combined to form the observed features
\end{itemize}

\greenbox{The latent variables $S_1,\dots,S_q$ account for the common sources of variance and their correlation
in the $x_1,\dots,x_p$, while the $\epsilon_j$ account for the remaining variance.}

\bluebox{Typically $S_j$ and $\epsilon_j$ are assumed to be Gaussian and uncorrelated.}

We assume
\begin{equation}
    \vec{S}_i \sim \mathcal{N}(0, \mat{1}), \quad \vec{\epsilon}_i \sim \mathcal{N}(0, \mat{\Psi})
\end{equation}

Note that for the $S_j$ to be jointly normal and uncorrelated ($\Cov(X,Y) = E[XY] - E[X] E[Y] = 0 \rightleftarrows E[XY] = E[X]E[Y]$), 
means they are independent ($p(x,y) = p(x) p(y)$).

\pinkbox{\textbf{Link to generative latent models:} Given the assumption $\vec{S}_i \sim \mathcal{N}(0, \mat{1})$, in this space
we can easily generate new samples of $\vec{S}_i$ by drawing from a normal distribution and given we have found the loading matrix
$\mat{\Gamma}$ and $\mat{\Psi}$, we can generate new samples of $\vec{x}_i$ as $\vec{x} = \vec{\mu} + \mat{\Gamma} \vec{S} + \vec{\epsilon}$ with $\vec{\epsilon}$ sampled from $\mathcal{N}(0, \mat{\Psi})$.}

\problem{Remember the discussed identifiability issue for the case of independent Gaussian sources - the user can search for 
rotated versions of the factors that are more easily interpretable.}

\greybox{\textbf{Difference to PCA:} PCA finds directions of maximum variance, Factor
Analysis finds underlying latent variables explaining common variance. 
Different from PCA, we also have the noise term $\vec{\epsilon}_i$ in the model. For Factor Analysis
we need the a priori knowledge to divide the total variance of a feature into common and unique variance.}

\subsubsection{Deriving the parameters in Factor Analysis by Maximum Likelihood}
We want to find the parameters $\vec{\mu}, \mat{\Gamma}, \mat{\Psi}$ by maximum likelihood.
Let us collect the parameters in a vector $\vec{\theta} = (\vec{\mu}, \mat{\Gamma}, \mat{\Psi})$ for 
shorthand notation.

We want to maximize the likelihood of the observed data given the parameters. Assuming the measurements to be i.i.d, we get

\begin{equation}
    \hat{\vec{\theta}}_{\text{MLE}} = \argmax_{\vec{\theta}} \prod_{i=1}^{N} p_{\vec{\theta}}(\vec{x}_i)
\end{equation}

As we do not know the latent variables $\vec{S}_i$, we have to integrate them out, so

\begin{equation}
    \begin{gathered}
        p_\vec{\theta}(\vec{x}) = \int_\vec{S} p_\vec{\theta} (\vec{x}, \vec{S}) \, d\vec{S} \underset{\text{Bayes}}{=} \int_\vec{S} p_\vec{\theta} (\vec{x} | \vec{S}) p(\vec{S}) \, d\vec{S} \\
        \text{usually difficult / intractable to calculate}
    \end{gathered}
\end{equation}

where in case of our factor analysis

\begin{equation}
    \vec{S}_i \sim \mathcal{N}(0, \mat{1}) \quad \rightarrow \quad p(\vec{S}_i) = (2\pi)^{-\frac{q}{2}} \exp \left( -\frac{1}{2} \vec{S}_i^T \vec{S}_i \right)
\end{equation}

and by our model

\begin{equation}
    \vec{x}_i | \vec{S}_i \sim \mathcal{N}(\vec{\mu} + \mat{\Gamma} \vec{S}_i, \mat{\Psi})
\end{equation}

so

\begin{equation}
    p_\vec{\theta}(\vec{x}_i | \vec{S}_i) = (2\pi)^{-\frac{p}{2}} \exp \left( -\frac{1}{2} (\vec{x}_i - \vec{\mu} - \mat{\Gamma} \vec{S}_i)^T \mat{\Psi}^{-1} (\vec{x}_i - \vec{\mu} - \mat{\Gamma} \vec{S}_i) \right)
\end{equation}

with \textit{factor loadings} $\mat{\Gamma}$, and factor scores $\vec{S}_i$.

\note{In a more general setting than factor analysis, $p_\vec{\theta}(\vec{x}_i | \vec{S}_i)$ might be a different model, e.g.
for $p$ neurons we want to find underlying latent variables $s_i$ modelling if they 
are firing $x_{ij} \in \{0,1\}, i =1,\dots,p, j = 1,\dots,N$ modelled by the Bernoulli distribution. 
\begin{equation}
    p(x_{ij} | s_i) = \Pi_i(s_i)^{x_{ij}} (1 - \Pi_i(s_i))^{1 - x_{ij}}
\end{equation}
with e.g. $\Pi_i(s_i) = (\operatorname{soft(arg)max} (s_1, \dots, s_p))_i$.
}

\problem{How to we go about the (intractable) integral over $\vec{S}_i$?}

\subsection{Generative Latent Models}
\subsubsection{Introduction and problem of the intractable marginal likelihood of the data}
Assume we have observations $\vec{x} \in \mathbb{R}^p$ in the data space $\mathcal{X}$. 
Also assume there is unobserved latent (or missing) 
data $\vec{z} \in \mathbb{R}^q$ in the latent space $\mathcal{Z}$.

\begin{itemize}
    \item Assume the distribution over the latent variables $\vec{z}$ is given
    by the prior distribution $p_{\vec{\theta}_{\text{lat}}}(\vec{z})$ with unknown parameters
    $\vec{\theta}_{\text{lat}}$.
    \item Assume a mapping from latent space to data space $\vec{f}: \mathcal{Z} \rightarrow \mathcal{X}$,
    and a noise model in the data space $p_{\vec{\theta}_{\text{obs}}}(\vec{x} | \vec{z})$ with unknown parameters
    $\vec{\theta}_{\text{obs}}$.
\end{itemize}

Let us write the parameters in one vector $\vec{\theta} = (\vec{\theta}_{\text{lat}}, \vec{\theta}_{\text{obs}})$.

\idea{If we could find the parameters $\vec{\theta}$ and so the distribution $p_\theta(\vec{z})$ we could sample data from the simpler
latent space and map it to the data space - generative modeling.}

\problem{At hand, we only have $N$ samples $\vec{x}_i$ in the data space, and we want to find the parameters $\vec{\theta}$}

By Bayes theorem, we can write

\begin{equation}
    p_\vec{\theta}(\vec{x},\vec{z}) = p_{\vec{\theta}_{\text{lat}}}(\vec{z}) p_{\vec{\theta}_{\text{obs}}}(\vec{x} | \vec{z})
\end{equation}

so the likelihood of the observed data given the parameters is

\begin{equation}
    p_\vec{\theta}(\vec{x}) = \int p_\vec{\theta}(\vec{x}, \vec{z}) \, d\vec{z} = \int p_{\vec{\theta}}(\vec{z}) p_{\vec{\theta}}(\vec{x} | \vec{z}) \, d\vec{z}
\end{equation}

\problem{This integral is intractable in general as it is performed over the whole latent space. More profoundly, while
me might use importance sampling to calculate this integral for given $\vec{\theta}$, how we integrate itself
(so what samples we take) depends in $\vec{\theta}$.}

But if we could calculate this marginal likelihood of the observed data,
the parameters would follow by

\begin{equation}
    \hat{\vec{\theta}}_{\text{MLE}} = \argmax_{\vec{\theta}} \prod_{i=1}^{N} p_{\vec{\theta}}(\vec{x}_i)
\end{equation}

\greybox{\textbf{Latent posterior:} Approximating the posterior $p_{\vec{\theta}}(\vec{z} | \vec{x})$ (the discriminative model)
would also be nice to do statistical inference in the latent space. Or assuming the latent variables are e.g. unobserved
class labels (so clustering), by the discriminative model (the latent posterior), we can make cluster assignments.}

\subsubsection{Evidence Lower Bound (ELBO)}
We need the marginal likelihood of the observed data,
$p_\vec{\theta}(\vec{x})$ for calculating the likelihood
(and from this the parameters $\vec{\theta}$) but it is intractable.

\idea{Do not maximize the log-likelihood directly but a lower bound on it, which is generally more tractable.}

Consider the logarithm of the marginal likelihood of the observed data

\begin{equation}
    \log p_\vec{\theta}(\vec{x}) = \log \int p_\vec{\theta}( \vec{x}, \vec{z}) \, d\vec{z}
\end{equation}

For some fixed $\vec{\theta}$ this is called the \textbf{evidence} of the model
(if this is high, we would assume that we have chosen a good model and good parameters).

\idea{Introduce a simple, easy to use variational density,
one we can easily sample from

\begin{equation}
    q_{\vec{\phi}(\vec{x})}(\vec{z})
\end{equation}

with parameters $\vec{\phi}$ which aims to approximate the more complicated true posterior $p_{\vec{\theta}}(\vec{z} | \vec{x})$.}

\note{The parametrization of the variational density $q$ depends on the data $\vec{x}$.}

Then one can find (proof follows) for any distribution $q_{\vec{\phi}(\vec{x})}(\vec{z})$ that

\begin{equation}
    \log p_\vec{\theta}(\vec{x}) \geq \text{ELBO}_{\vec{\phi},\vec{x}}(\vec{\theta}) := E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}} \left[ \log \frac{p_\vec{\theta}(\vec{x},\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} \right]
\end{equation}

\greenbox{This can be estimated using Monte Carlo importance sampling. For samples $\vec{z}_1,\dots,\vec{z}_M \sim q_{\vec{\phi}(\vec{x})}(\vec{z})$ we can estimate
\begin{equation}
    \text{ELBO}_{\vec{\phi},\vec{x}}(\vec{\theta}) \approx \frac{1}{M} \sum_{m=1}^{M} \log \frac{p_\vec{\theta}(\vec{x},\vec{z}_m)}{q_{\vec{\phi}(\vec{x})}(\vec{z}_m)}
\end{equation}
(which by Jenssen's inequality is biased downwards).}

\greenbox{So what have we won in comparison to the original problem?
\begin{itemize}
    \item the samples from $q_{\vec{\phi}(\vec{x})}(\vec{z})$ do not depend on $\vec{\theta}$, so we can optimize the ELBO, different from
     the original $\int p_{\vec{\theta}}(\vec{z}) p_{\vec{\theta}}(\vec{x} | \vec{z}) \, d\vec{z}$ where an update to $\theta$ would
     have made new samples necessary.
     \item if we also optimize the variational density $q_{\vec{\phi}(\vec{x})}(\vec{z})$, we can make it more similar to the true posterior,
     so we get a posterior in the latent space from which we can easily sample and generate new data by the mapping $\vec{f}$. Then
     we can also approximate
     \begin{equation}
        p_\vec{\theta}( \vec{x}) = \frac{p_\vec{\theta}(\vec{x}|\vec{z}) p(\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})}
     \end{equation}
     so not by the integral as before.
\end{itemize}
}

\subsubsubsection{Proof of the ELBO}
We have
\begin{equation}
    \log p_\vec{\theta}(\vec{x}) = \log \int_{\vec{z}} q_{\vec{\phi}(\vec{x})}(\vec{z}) \frac{p_\vec{\theta}(\vec{x},\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} \, d\vec{z} \underset{\text{Jenssen's inequality}}{\geq} \int_{\vec{z}} q_{\vec{\phi}(\vec{x})}(\vec{z}) \log \frac{p_\vec{\theta}(\vec{x},\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} \, d\vec{z}
\end{equation}
where Jenssens inequality holds because $\log$ is concave.

\pinkbox{We see that if $q_{\vec{\phi}(\vec{x})}(\vec{z}) = p_{\vec{\theta}}(\vec{z} | \vec{x})$, then the ELBO is equal to the log-likelihood, as by the \textit{path-rule}
\begin{equation}
    \frac{p_\vec{\theta}(\vec{x},\vec{z})}{p_\vec{\theta}(\vec{z}|\vec{x})} = p_\vec{\theta}(\vec{x})
\end{equation}
}

\textbf{On Jenssen's inequality}: Jenssens inequalities are given in 
table \ref{tab:jenssen}.

\begin{table}[!htb]
    \centering
    \begin{tabular}{|p{0.3\textwidth}||p{0.3\textwidth}|p{0.3\textwidth}|}
        \hline
        & \textcolor{blue1}{$f$ concave} ($\forall x: \partial_x^2 f \leq 0$), e.g. $f:x\mapsto \log x$ & \textcolor{blue1}{$f$ convex} ($\forall x: \partial_x^2 f \geq 0$), e.g. $f:x\mapsto x^2$  \\
        \hline
        discrete case with $\alpha_i \geq 0, \sum \alpha_i = 1$ & $f(\sum \alpha_i x_i) \geq \sum \alpha_i f(x_i)$ & $f(\sum \alpha_i x_i) \leq \sum \alpha_i f(x_i)$ \\
        \hline
        continuous case with $\alpha(x) \geq 0, \int \alpha(x) \, dx = 1$ & $f(\int \alpha(x) g(x) \, dx) \geq \int \alpha(x) f(g(x)) \, dx$ & $f(\int \alpha(x) g(x) \, dx) \leq \int \alpha(x) f(g(x)) \, dx$ \\
        \hline
        application to the expectation value & $f(E[X]) \geq E[f(X)]$ & $f(E[X]) \leq E[f(X)]$ \\
        \hline
    \end{tabular}
    \caption{Jenssen's inequalities.}
    \label{tab:jenssen}
\end{table}

The discrete concave variant in 1D states, that the secant line between two points on a concave function is always below the function, so
\begin{equation}
    f(\alpha x_1 + (1-\alpha) x_2) \geq \alpha f(x_1) + (1-\alpha) f(x_2), \quad \alpha \in [0,1]
\end{equation}
(which is really just the proper definition of a concave function).

This is illustrated in figure \ref{fig:jenssen}.

\begin{figure}[H]
    \centering
    \includesvg[width=0.9\textwidth]{figures/jenssen.svg}
    \caption{Illustration of Jenssen's inequality.}
    \label{fig:jenssen}
\end{figure}

\subsubsubsection{Gap between evidence and ELBO}
The gap between the evidence
\begin{equation}
    \text{evidence} := \log p_\vec{\theta}(\vec{x})
\end{equation}
and the evidence lower bound (ELBO)
\begin{equation}
    \text{ELBO}_{\vec{\phi},\vec{x}}(\vec{\theta}) = E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}} \left[ \log \frac{p_\vec{\theta}(\vec{x},\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} \right]
\end{equation}
turns out to be the Kullback-Leibler divergence between the true posterior and the variational density
\begin{equation}
    \begin{aligned}
        \operatorname{KL}(q_{\vec{\phi}(\vec{x})}(\vec{z}) || p_{\vec{\theta}}(\vec{z} | \vec{x})) &= E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}} \left[ \log \frac{q_{\vec{\phi}(\vec{x})}(\vec{z})}{p_{\vec{\theta}}(\vec{z} | \vec{x})} \right] \\
        &= E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}} \left[ \log q_{\vec{\phi}(\vec{x})}(\vec{z})\right] - E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}} \left[ \log \underbrace{p_{\vec{\theta}}(\vec{z} | \vec{x})}_{=\, \frac{p_\vec{\theta}(\vec{x},\vec{z})}{p_\vec{\theta}(\vec{x})}} \right] \\
        &= E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}} \left[ \log q_{\vec{\phi}(\vec{x})}(\vec{z})\right] - E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}} \left[ \log p_{\vec{\theta}}(\vec{x},\vec{z}) \right] + \log p_\vec{\theta}(\vec{x}) \\
        &= \log p_\vec{\theta}(\vec{x}) - \text{ELBO}_{\vec{\phi},\vec{x}}(\vec{\theta}) \\
        &= \text{evidence} - \text{ELBO}
    \end{aligned}
\end{equation}

So while the direct calculation of the posterior $p_{\vec{\theta}}(\vec{z} | \vec{x})$ is intractable, we can
approximate it by the variational density $q_{\vec{\phi}(\vec{x})}(\vec{z})$ when we maximize the ELBO, under
variation of $\vec{\phi}$.

This is illustrated in figure \ref{fig:elbo_gap}.

\begin{figure}[!htb]
    \centering
    \includesvg[width=0.9\textwidth]{figures/elbo_gap.svg}
    \caption{Illustration of the gap between the evidence and the ELBO.}
    \label{fig:elbo_gap}
\end{figure}

\greenbox{So maximizing ELBO, gives us both the generative model $p_\vec{\theta}(\vec{x}, \vec{z})$ and the discriminative model $p_{\vec{\theta}}(\vec{z} | \vec{x}) \approx q_{\vec{\phi}(\vec{x})}(\vec{z})$.}

\subsubsection{Variational Expectation-Maximization (EM) algorithm}
Based on the introduction of ELBO, the following aims are natural
\bluebox{\textbf{Aims}:
\begin{itemize}
    \item find $q_{\vec{\phi}(\vec{x})}(\vec{z})$ so that it is close to the true posterior $p_{\vec{\theta}}(\vec{z} | \vec{x})$
    \item find parameters $\vec{\theta}$ (our model parameters) that maximize the ELBO
\end{itemize}
}
Doing this simultaneously is intractable, so we alternate between updating 
the variational density $q_{\vec{\phi}(\vec{x})}(\vec{z})$ and the model parameters $\vec{\theta}$.

The EM algorithm consists of (here given as a maximization-maximization procedure)
\begin{itemize}
    \item \textbf{Expectation-step:} Our baseline problem is that we do not know the distribution of the
    latent variables given our observations. So given a current estimate of the model parameters $\vec{\theta}^*$, find a possible
    distribution $q_{\vec{\phi}(\vec{x})}(\vec{z})$ that is close to the true posterior $p_{\vec{\theta}}(\vec{z} | \vec{x})$
    so optimally maximizing the ELBO
    \begin{equation}
        \vec{\phi}^*(\vec{x}_i) = \argmax_{\vec{\phi}(\vec{x}_i)} \text{ELBO}(\vec{\theta}^*, q_{\vec{\phi}(\vec{x}_i)}), \quad i = 1,\dots,N
    \end{equation}
    We might not do this maximization explicitly, but ask ourselves what latent distribution would be reasonable given our current parameters.
    With this we can calculate
    \begin{equation}
        \text{ELBO}_{\vec{\phi},\vec{x}_i}(\vec{\theta}) = E_{\vec{z} \sim q_{\vec{\phi}^*(\vec{x}_i)}} \left[ \log \frac{p_\vec{\theta}(\vec{x}_i,\vec{z})}{q_{\vec{\phi}^*(\vec{x}_i)}(\vec{z})} \right]
    \end{equation}
    which is then \textbf{only a function of $\vec{\theta}$}.
    \item \textbf{Maximization-step:} Assume the above expectation was calculated under some $\vec{\phi}^*(\vec{x}_i)$ reasonably good, find
    the model parameters $\vec{\theta}$ that maximize the ELBO (under i.i.d. assumptions)
    \begin{equation}
        \begin{aligned}    
            \vec{\theta}^* &= \argmax_{\vec{\theta}} \sum_{i = 1}^{N} \text{ELBO}_{\vec{\phi}^*(\vec{x}_i),\vec{x}_i}(\vec{\theta}) \\
                           &= \argmax_{\vec{\theta}} \sum_{i = 1}^{N} E_{\vec{z} \sim q_{\vec{\phi}^*(\vec{x}_i)}} \left[ \log \frac{p_\vec{\theta}(\vec{x}_i,\vec{z})}{q_{\vec{\phi}^*(\vec{x}_i)}(\vec{z})} \right] \\
                           &= \argmax_{\vec{\theta}} \sum_{i = 1}^{N} E_{\vec{z} \sim q_{\vec{\phi}^*(\vec{x}_i)}} \left[ \log p_\vec{\theta}(\vec{x}_i,\vec{z}) \right] - \explain{E_{\vec{z} \sim q_{\vec{\phi}^*(\vec{x}_i)}} \left[ \log q_{\vec{\phi}^*(\vec{x}_i)}(\vec{z}) \right]}{does not depend on $\vec{\theta}$} \\
                           &= \argmax_{\vec{\theta}} \sum_{i = 1}^{N} E_{\vec{z} \sim q_{\vec{\phi}^*(\vec{x}_i)}} \left[ \log p_\vec{\theta}(\vec{x}_i,\vec{z}) \right]
        \end{aligned}
    \end{equation}
    motivated by $\log p_\vec{\theta}(\vec{x}) \geq \text{ELBO}_{\vec{\phi},\vec{x}}(\vec{\theta})$.
\end{itemize}

Starting with initial estimates $\vec{\theta}^{(0)}, \vec{\phi}^{(0)}$, the EM algorithm iterates
until $\Delta \text{ELBO} < \epsilon$ for some small $\epsilon$.

\note{This is not guaranteed to converge but converges for some problems.}

\note{Often this is formulated in terms of distributions over the whole data sets and sets of latent variables.}

As of our estimated parameters, $p_\vec{\theta}(\vec{z})$ and $p_\vec{\theta}(\vec{x} | \vec{z})$ are known
(tractable distributions in the first place) (mind that $\vec{\theta}$ is the collection
of parameters of both) and the intractable latent posterior

\begin{equation}
    p_\vec{\theta}(\vec{z} | \vec{x}) \approx q_{\vec{\phi}(\vec{x})}(\vec{z})
\end{equation}

and the intractable marginal likelihood of the data

\begin{equation}
    p_\vec{\theta}(\vec{x}) = \int p_\vec{\theta}(\vec{x}, \vec{z}) \, d\vec{z} \approx \frac{p_\vec{\theta}(\vec{x}|\vec{z})p_\vec{\theta}(\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})}
\end{equation}

can be approximated.

\subsubsection{Application of the EM algorithm: Gaussian Mixture Models (GMM)}
Consider we want to estimate the probability distribution of data $\mat{X}$ by 
a mixture of Gaussians. This is illustrated in figure \ref{fig:gmm}.

\bluebox{In this case our latent variables are assignments of data points to specific Gaussians,
so to specific clusters.}

\begin{figure}[!htb]
    \centering
    \includesvg[width=0.9\textwidth]{figures/gauss_mix.svg}
    \caption{Illustration of a Gaussian Mixture Model.}
    \label{fig:gmm}
\end{figure}

Given the unobserved class, our likelihood model is

\begin{equation}
    \vec{x} \mid k \sim \mathcal{N}( \vec{\mu}_k, \mat{\Sigma}_k)
\end{equation}

and let $\Pi_k$ be the unknown prior probability of class $k$\footnote{If we knew class
labels as in discriminant analysis this would be the ratio of data points of class $k$
to all data points - but we do not have class labels.}.

Our model for the likelihood of a data point $\vec{x}$ with marginalized out class labels is

\begin{equation}
    p_\vec{\theta}(\vec{x}) = \sum_{k=1}^{K} p(\vec{x}, k) = \sum_{k=1}^{K} \Pi_k \mathcal{N}(\vec{x} | \vec{\mu}_k, \mat{\Sigma}_k), \quad \sum_{k=1}^{K} \Pi_k = 1
\end{equation}

(other basic models, e.g. Bernoulli would also be possible) where

\begin{equation}
    \vec{\theta} = \{ \Pi_1, \vec{\mu}_1, \mat{\Sigma}_1, \dots, \Pi_K, \vec{\mu}_K, \mat{\Sigma}_K \}
\end{equation}

and the (logarithmic) likelihood of a training-set $\mathcal{D} = \{ \vec{x}_1, \dots, \vec{x}_N \}$

\begin{equation}
    \begin{gathered}
        \log \mathcal{L}_{\vec{\theta}}(\mathcal{D}) = \log p_{\vec{\theta}}(\mathcal{D}) = \log \prod_{i=1}^{N} p_\vec{\theta}(\vec{x}_i) = \sum_{i=1}^{N} \log p_\vec{\theta}(\vec{x}_i) = \sum_{i=1}^{N} \log \sum_{k=1}^{K} \Pi_k p(\vec{x}_i | k, \vec{\theta}_k) \\
        p(\vec{x}_i | k, \vec{\theta}_k) = \mathcal{N}(\vec{x}_i | \vec{\mu}_k, \mat{\Sigma}_k)
    \end{gathered}
\end{equation}

\problem{The sum in the logarithm prevents us from applying the logarithm to the summands, the normal densities. So no easy
path towards a closed form solution.}

\idea{Imagine we would know the discriminant model $\phi_{ik} = p(k|\vec{x}_i)$, then a sensible closed form approach
for the parameters would be
\begin{equation}
    \begin{aligned}
        N_k &= \sum_{i=1}^{N} \phi_{ik} \quad \# \text{ points effectively assigned to cluster $k$} \\
        \Pi_k &= \frac{N_k}{N} \\
        \vec{\mu}_k &= \frac{1}{N_k} \sum_{i=1}^{N} \phi_{ik} \vec{x}_i \\
        \mat{\Sigma}_k &= \frac{1}{N_k} \sum_{i=1}^{N} \phi_{ik} (\vec{x}_i - \vec{\mu}_k)(\vec{x}_i - \vec{\mu}_k)^T
    \end{aligned}
\end{equation}}

\idea{Let us just approximate $\phi_{ik} = p(k|\vec{x}_i)$.}

Introduce the latent variables $\vec{z}_i \in \{0,1\}^K$ with $z_{ik} = 1$ if $\vec{x}_i$ is assigned to the $k$-th Gaussian
(1-hot encoded).

\paragraph*{E-step - updating the approximated discriminant distribution over $\vec{z}_i | \vec{x}_i$, $q_{\vec{\phi}(\vec{x})}(\vec{z}_i)$} Naturally, the probability that $\vec{x}_i$ is assigned to the $k$-th Gaussian is given by
the probability of that Gaussian at $\vec{x}_i$ (with its weighting as in the density approximation) divided by the sum of the probabilities of all Gaussians at $\vec{x}_i$ (with their respective
general weightings),
\begin{equation}
    \begin{aligned}
        \forall i,k: \quad p(z_{ik} = 1 | \vec{x}_i, \vec{\theta}^{(N)}) &= \frac{p(\vec{x}_i|\vec{z};\vec{\theta}^{(n)}) p(z_{ik} = 1 | \vec{\theta}^{(n)})}{p(\vec{x}_i | \vec{\theta}^{(n)})} \\
        &= \frac{\Pi_k \mathcal{N}(\vec{x}_i | \vec{\mu}_k, \mat{\Sigma}_k)}{\sum_{l=1}^{K} \Pi_l \mathcal{N}(\vec{x}_i | \vec{\mu}_l, \mat{\Sigma}_l)}
    \end{aligned}
\end{equation}
where here at each point we have the posterior categorical distribution
\begin{equation}
    q_{\vec{\phi}(\vec{x}_i)}(\vec{z}_i) = \prod_{k=1}^{K} \left( \phi_{ik} \right)^{z_{ik}}, \quad \phi_{ik} = p\left(z_{ik} = 1 | \vec{x}_i, \vec{\theta}^{(N)}\right)
\end{equation}

\greenbox{\textbf{Closed form successive approximation procedure}: With these $\phi_{ik}$ we can calculate the model parameters as given above,
with these new model parameters, we can calculate new $\phi_{ik}$, and so on.
\enquote{One man's vicious circle is another man's successive approximation procedure.} (Cosma Shalizi)}

Below we give the formal $M$-step, although we have already given a reasonable succesive procedure.

\paragraph*{M-step - updating the model parameters} We want to find the model parameters that maximize the ELBO. As an approximation
to the true posterior $p_{\vec{\theta}}(\vec{z} | \vec{x})$, we use the variational density $q_{\vec{\phi}(\vec{x})}(\vec{z})$.

\begin{equation}
    \begin{aligned}
        \vec{\theta}^{(n+1)} &= \argmax_{\vec{\theta}} \sum_{i=1}^{N} \text{ELBO}_{\vec{\phi}(\vec{x}_i),\vec{x}_i}(\vec{\theta}) \\
        &= \argmax_{\vec{\theta}} \sum_{i=1}^{N} \sum_{k=1}^{K} \explain{\phi_{ik}}{from E-step} \log p_\vec{\theta}(\vec{x}_i,\vec{z}_i) \\
        &= \argmax_{\vec{\theta}} \sum_{i=1}^{N} \sum_{k=1}^{K} \phi_{ik} \left[ \log p_\vec{\theta}(\vec{x}_i | \vec{z}_i) + \log p_\vec{\theta}(\vec{z}_i) \right] \\
        &= \argmax_{\vec{\theta}} \sum_{i=1}^{N} \sum_{k=1}^{K} \phi_{ik} \left[ \log \mathcal{N}(\vec{x}_i | \vec{\mu}_k, \mat{\Sigma}_k) + \log \prod_{r=1}^{K} \Pi_l^{z_{ir}} \right]
    \end{aligned}
\end{equation}

We maximize this in $\vec{\theta} = \{ \Pi_1, \vec{\mu}_1, \mat{\Sigma}_1, \dots, \Pi_K, \vec{\mu}_K, \mat{\Sigma}_K \}$ to find the next
iteration of the model parameters $\vec{\theta}^{(n+1)}$.

\greybox{This is somewhat similar to K-means, where we assign data points to the nearest cluster and then update the cluster centers to the center of mass of the assigned data points.}

\pinkbox{\textbf{Gaussian mixture as a generative model:}
\begin{enumerate}
    \item From the categorical distribution of the $\Pi_l, l = 1,\dots,K$, we can sample the class of a new data point
    \item Given the class, we can sample a new data point from the Gaussian distribution with the parameters $\vec{\mu}_l, \mat{\Sigma}_l$
\end{enumerate}}

\bluebox{In general the generative process is
\begin{equation}
    \vec{z} \sim p_\vec{\theta}(\vec{z}) \quad \rightarrow \quad \vec{x} \sim p_\vec{\theta}(\vec{x} | \vec{z})
\end{equation}}

\note{The Gaussian mixture model with the EM algorithm can not itself find the number of clusters $K$ in the data, use e.g.
cross-validation for this.}

\subsubsection{Variational Inference}
\subsubsubsection{Recapitulation on parameter estimation | parameters vs. latent variables}
Let us go back a step. Previously, we had a model
for the likelihood given parameters $p(\vec{x}|\vec{\theta})$ - for instance a Gaussian with
given parameters $\mu$ and $\sigma$ so over our 
whole dataset $\mathcal{D} = \{ \vec{x}_i, \dots, \vec{x}_N \}$ under i.i.d.
assumptions the likelihood
\begin{equation}
    p(\mathcal{D}|\vec{\theta}) = \mathcal{L}_\mathcal{D}(\vec{\theta}) = \prod_{i = 1}^{N} p(\vec{x}_i|\vec{\theta})
\end{equation}
where
\begin{itemize}
    \item in the \textit{frequentist} MLE approach we would find a point-estimate of the parameters
    maximizing the likelihood
    \item and in the Bayesian setting, we would assume a prior distribution $p(\vec{\theta})$ (and as before the likelihood) and 
    by Bayes theorem calculate a full distribution over the parameters
    \begin{equation}
        p(\vec{\theta}|\mathcal{D}) = \frac{p(\mathcal{D}|\vec{\theta}) p(\vec{\theta})}{\int p(\vec{\theta}) p(\mathcal{D}|\vec{\theta}) d\vec{\theta}}
    \end{equation}
    Where as of the \textit{evidence integral} this posterior would usually be intractable, but we can sample from it using Marcov Chain Monte Carlo (MCMC)
    (where the evidence drops out), and thus calculate integrals over the posterior, e.g. the first moment, the posterior mean (just the mean over the 
    MCMC samples). Note that also here, the prior and posterior are distributions which themselves have parameters, not explicit here
    which would be updated in a Bayesian update step.
\end{itemize}

\pinkbox{\textbf{Latent variables vs. parameters:} Parameters are often 
connoted to the idea of having fixed values, not being random variables with
a distribution. In the above Bayesian setting it therefore makes more sense
to think of the \textit{parameters} as \textbf{latent variables}.}

\subsubsubsection{The general problem of inference: Sampling vs. Variational approach}
We have
\begin{itemize}
    \item data $\mathcal{D} = \{ \vec{x}_i, \dots, \vec{x}_N \}$
    \item a prior on unobserved / hidden latent variables $\vec{z}$, e.g. parameters $\vec{\theta}$, e.g. flat if we have no prior knowledge
    \item a model of the likelihood $p(\vec{x}|\vec{z})$, which in the general latent setting might need more parameters
    to be specified (finding which one can e.g. use expectation maximization), or if we are in the above inference setting
    is fully given by our parameters of interest $\vec{z} = \vec{\theta}$, those we are looking for
\end{itemize}

Our joint model is
\begin{equation}
    p(\vec{z},\vec{x}) = p(\vec{z}) p(\vec{x}|\vec{z})
\end{equation}
where the latent variables help govern the distribution of the data.

\bluebox{\textbf{Aim}: Given a measurement $\vec{x}$ we are interested in the posterior (discriminative model)
$p(\vec{z}|\vec{x})$ so in the parameter inference setting $p(\vec{\theta}|\vec{x})$ (or
rather this posterior given the full dataset).}

The main two approaches are
\begin{itemize}
    \item \textbf{Sampling Techniques:} We can sample from $p(\vec{z}|\vec{x})$ by MCMC (which only requires knowing the
    likelihood and prior, the evidence drops out) and construct an empirical estimate of the posterior.
    \begin{itemize}
        \item \textcolor{green1}{this is unbiased in the sense that we make no distributional assumption on the posterior}
        \item \textcolor{red1}{this might be too costly - trillions of MCMC samples might be necessary}
    \end{itemize}
    \item \textbf{Variational Inference}: Use optimization rather than sampling. Use an approximate
    density from some family $q_{\vec{\phi}(\vec{x})}(\vec{z}) \in \mathcal{Q}$ meant to approximate the posterior (ideally expressive and tractable). Find the family-member
    best approximating the true posterior by minimizing the Kullback-Leibler divergence
    \begin{equation}
        q_{\vec{\phi}(\vec{x})}^*(\vec{z}) = \argmin_{q_{\vec{\phi}(\vec{x})}(\vec{z}) \in \mathcal{Q}} D_{KL}(q_{\vec{\phi}(\vec{x})}(\vec{z})||p(\vec{z}|\vec{x}))
    \end{equation}
    which we do by maximizing the evidence lower bound as before.
\end{itemize}

\greybox{\textbf{When to use MCMC, when variational methods?:} MCMC methods tend to
be computationally intensive but guarantee producing asymptotically exact samples
from the target density. Variational inference (VI) does not have these guarantees but
is usually faster.
\begin{itemize}
    \item use MCMC when: smaller dataset, higher computational cost for more precise samples is fine
    \item use VI when: large data sets, quickly want to explore many models, so when speed is requested, posterior is sufficiently simple to be modeled by the given family $\mathcal{Q}$
\end{itemize}
}

The comparison of the two methods is illustrated in figure \ref{fig:vi_mcmc}.

\begin{figure}[!htb]
    \centering
    \includesvg[width=0.9\textwidth]{figures/mc_vi.svg}
    \caption{Comparison of variational inference and MCMC.}
    \label{fig:vi_mcmc}
\end{figure}

\subsubsubsection{Variational inference}
\idea{Approximate the posterior $p(\vec{z}|\vec{x})$ by $q_{\vec{\phi}(\vec{x})}(\vec{z}) \in \mathcal{Q}$}
\begin{equation}
    \begin{aligned}
        q_{\vec{\phi}(\vec{x})}^*(\vec{z}) &= \argmin_{q_{\vec{\phi}(\vec{x})}(\vec{z}) \in \mathcal{Q}} D_{KL}(q_{\vec{\phi}(\vec{x})}(\vec{z})||p(\vec{z}|\vec{x})) \\
        &= \argmin_{q_{\vec{\phi}(\vec{x})}(\vec{z}) \in \mathcal{Q}} \left( \int q_{\vec{\phi}(\vec{x})}(\vec{z}) \log \frac{q_{\vec{\phi}(\vec{x})}(\vec{z})}{p(\vec{z}|\vec{x})} d\vec{z} \right) \\
        &= \argmin_{q_{\vec{\phi}(\vec{x})}(\vec{z}) \in \mathcal{Q}} \left( \int q_{\vec{\phi}(\vec{x})}(\vec{z}) \log \frac{q_{\vec{\phi}(\vec{x})}(\vec{z}) p(\vec{x})}{p(\vec{z},\vec{x})} d\vec{z} \right) \\
        &= \argmin_{q_{\vec{\phi}(\vec{x})}(\vec{z}) \in \mathcal{Q}} \left( \underbrace{\int q_{\vec{\phi}(\vec{x})}(\vec{z}) \log p(\vec{x}) d\vec{z}}_{=\,\text{evidence } \log p(\vec{x})} - \underbrace{\int q_{\vec{\phi}(\vec{x})}(\vec{z}) \log \frac{p(\vec{z},\vec{x})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} d\vec{z}}_{=\text{ELBO}(q_{\vec{\phi}(\vec{x})})} \right) \\
        &= \argmax_{q_{\vec{\phi}(\vec{x})}(\vec{z}) \in \mathcal{Q}} \text{ELBO}(q_{\vec{\phi}(\vec{x})})
    \end{aligned}
\end{equation}

\subsubsubsection{Mean-Field Algorithm}
Here we assume the variational density (the approximate posterior) to factorize over the latent variables
\begin{equation}
    q_{\vec{\phi}(\vec{x})}(\vec{z}) = \prod_{i=1}^{q} q_i(z_i)
\end{equation}
(or at least to be partitionable), where for the $q_i$ one can find
the best approximation to the true posterior by calculus of variations,
by distributional assumptions, an explicit form for the best $q_i$ can
often be found.

\subsubsection{Variational Autoencoder (VAE)}

\greenbox{While a deterministic autoencoder just computes embeddings of input vectors, the \textbf{variational autoencoder
is a generative model}.}

\idea{Describe the variational parameters by $\vec{\phi}(\vec{x}) = \operatorname{NN}_{\vec{\Omega}_{\text{Enc}}}(\vec{x})$ so using
a neural network and also stuff the $z$-dependency of $p(\vec{x}|\vec{z})$ into $z$-dependent parameters $\vec{\theta}(\vec{z}) = \operatorname{NN}_{\vec{\Omega}_{\text{Dec}}}(\vec{z})$.}

\subsubsubsection{Back to the problem of likelihood optimization under latent variables}
Let us go back to the problem of maximizing the likelihood
$p_\vec{\theta}(\vec{x}) = p(\vec{x}|\vec{\theta})$.
If this likelihood is sufficiently simple, we can find 
parameters $\vec{\theta}$ by usual MLE.

\problem{What if we have further latent, unobserved variables, e.g.
unobserved class labels?}

We marginalize them out by

\begin{equation}
    p_\vec{\theta}(\vec{x}) = \int p_\vec{\theta}(\vec{x},\vec{z}) \, d\vec{z} \underset{\text{chain rule}}{=} \int p_\vec{\theta}(\vec{x}|\vec{z}) p_\vec{\theta}(\vec{z}) \, d\vec{z}
\end{equation}

e.g. $p_\vec{\theta}(\vec{x}|\vec{z})$ is a Gaussian and thus $p_\vec{\theta}(\vec{x})$ is a mixture of Gaussians.

\problem{\begin{itemize}
    \item This integral is generally problematic, more so if we want to optimize $\theta$ through it.
    \item The posterior $p_\vec{\theta}(\vec{z}|\vec{x})$ is expensive / intractable to calculate.
\end{itemize}}

\idea{As previously, we introduce the approximation
\begin{equation}
    q_{\vec{\phi}(\vec{x})}(\vec{z}) \underset{\text{goal}}{\approx} p_{\vec{\theta}}(\vec{z}|\vec{x})
\end{equation}
Finding a good approximation $q_{\vec{\phi}(\vec{x})}(\vec{z})$ later amortizes, because we can then
infer a $\vec{z}$ from a $\vec{x}$ without doing any integrals (\textit{amortized inference}).
}

Also as previously, we find good parameters $\vec{\theta}^*$ and $\vec{\phi}^*$ by maximizing the ELBO
\begin{equation}
    \vec{\theta}^*, \vec{\phi}^*(\vec{x}) = \argmax_{\vec{\theta}, \vec{\phi}(\vec{x})} \text{ELBO}_{\vec{\theta}, \vec{\phi}}(\vec{x})
\end{equation}
where a high evidence lower bound is wished for as
\begin{equation}
    \text{ELBO}_{\vec{\theta}, \vec{\phi}}(\vec{x}) = E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}(\vec{z})} \left[ \log \frac{p_\vec{\theta}(\vec{x},\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} \right] = \explain{\log p_\vec{\theta}(\vec{x})}{quality of the fit of the model to the data} - \explain{D_{KL}(q_{\vec{\phi}(\vec{x})}(\vec{z}) || p_{\vec{\theta}}(\vec{z}|\vec{x}))}{correctness of latent model}
\end{equation}
\note{The notation $\text{ELBO}_{\vec{\theta}, \vec{\phi}}(\vec{x})$ makes the dependence on $\vec{x}$ more explicit than
previously. The distributions $q_{\vec{\phi}(\vec{x})}(\vec{z})$ are normally parameterized for each individual data point in a separate optimization process.
More explicitly, one would always write $q_{\vec{\phi}(\vec{x})}(\vec{z}|\vec{x})$.}

\subsubsubsection{The Variational Autoencoder}
We now take a probabilistic approach to the autoencoder
\begin{itemize}
    \item \textbf{Probabilistic Encoder:} The encoder $h_{\vec{\Omega}_{\text{Dec}}}(\vec{x})$ takes input points from the normal data space $\vec{x}$ and outputs
    parameters $\vec{\phi}$ for the variational density $q_{\vec{\phi}(\vec{x})}(\vec{z})$. One can then
    for this location $\vec{x}$ sample a $\vec{z}$ from $q_{\vec{\phi}(\vec{x})}(\vec{z})$ with the
    obtained parameters. 
    \item \textbf{Probabilistic Decoder:} The decoder $f_{\vec{\Omega}_{\text{Dec}}}(\vec{z})$ maps from the latent space $\vec{z}$ to 
    to parameters $\vec{\theta}$ for the likelihood $p_{\vec{\theta}}(\vec{x}|\vec{z})$, from 
    which we can sample a $\vec{x}$. (Only mapping to the mean of the likelihood is also possible.)
\end{itemize}

\greenbox{While the distributions $q_{\vec{\phi}(\vec{x})}(\vec{z})$ and $p_{\vec{\theta}}(\vec{x}|\vec{z})$ might generally
be very simple, as of the possibly complex mappings $h_{\vec{\Omega}_{\text{Dec}}}$ and $f_{\vec{\Omega}_{\text{Dec}}}$, the
variational autoencoder can model very complex distributions.}

The general variational autoencoder idea is illustrated in figure \ref{fig:vae}.

\begin{figure}[!htb]
    \centering
    \includesvg[width=0.95\textwidth]{figures/varauto.svg}
    \caption{Illustration of the variational autoencoder idea.}
    \label{fig:vae}
\end{figure}

\subsubsubsection{Gradient of the ELBO}
The ELBO is given by
\begin{equation}
    \text{ELBO}_{\vec{\theta}, \vec{\phi}}(\vec{x}) = E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}(\vec{z})} \left[ \log \frac{p_\vec{\theta}(\vec{x},\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} \right]
\end{equation}
For a given $\vec{x}$, we calculate the parameters $\vec{\phi}$ for the variational density $q_{\vec{\phi}(\vec{x})}(\vec{z})$ by the encoder
${\vec{\phi}} = h_{\vec{\Omega}_{\text{Dec}}}(\vec{x})$ and draw
\begin{equation}
    \vec{z}^{(l)} \sim q_{\vec{\phi}(\vec{x})}(\vec{z}), \quad l = 1,\dots,L
\end{equation}
With the decoder we calculate $\vec{\theta} = f_{\vec{\Omega}_{\text{Dec}}}(\vec{z}^{(l)})$. With this 
an estimate of the ELBO is given by

\begin{equation}
    \text{ELBO}_{\vec{\theta}, \vec{\phi}}(\vec{x}) \approx \frac{1}{L} \sum_{l=1}^{L} \left(\log p_{\vec{\theta}}(\vec{x}|\vec{z}^{(l)}) - \log q_{\vec{\phi}(\vec{x})}(\vec{z}^{(l)})\right)
\end{equation}
\bluebox{For being able to backpropagate to $\vec{\Omega}_{\text{Dec}}$ and $\vec{\Omega}_{\text{Dec}}$, we need to calculate the gradient of the ELBO with respect to the parameters
\begin{equation}
    \partial_{\vec{\theta}, \vec{\phi}} \text{ELBO}_{\vec{\theta}, \vec{\phi}}(\vec{x})
\end{equation}
}

\note{In other articles, $\theta$ and $\phi$ are often used to denote the parameters of the encoder and decoder, respectively (as the
parameters of the variational density and the likelihood follow from them).}

We can readily calculate
\begin{equation}
    \partial_\vec{\theta} \text{ELBO}_{\vec{\theta}, \vec{\phi}}(\vec{x}) = \partial_\vec{\theta} E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}(\vec{z})} \left[ \log \frac{p_\vec{\theta}(\vec{x},\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} \right] = E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}(\vec{z})} \left[ \partial_\vec{\theta} \log p_\vec{\theta}(\vec{x}|\vec{z}) \right]
\end{equation}

\problem{In the calculation
\begin{equation}
    \partial_{\vec{\phi}} \text{ELBO}_{\vec{\theta}, \vec{\phi}}(\vec{x}) = \partial_{\vec{\phi}} E_{\vec{z} \sim q_{\vec{\phi}(\vec{x})}(\vec{z})} \left[ \log \frac{p_\vec{\theta}(\vec{x},\vec{z})}{q_{\vec{\phi}(\vec{x})}(\vec{z})} \right]
\end{equation}
we cannot put the derivative inside the expectation, because this expectation (how we sample the $\vec{z}$) depends on the parameters $\vec{\phi}$ itself.
}

\greenbox{\textbf{Reparametrization trick}: Write $z^{(l)}(\vec{x})$ as a deterministic function depending on $\vec{\phi}$ and a parameter-free random variable $\epsilon^{(l)}$ which is independent of $\vec{\phi}$.

\begin{equation}
    z^{(l)} = g_{\vec{\phi}(\vec{x})}(\epsilon^{(l)}, \vec{x})
\end{equation}

for instance assume

\begin{equation} 
    z^{(l)} \mid \vec{x} \sim \mathcal{N}(\mu_{\vec{\phi}(\vec{x})}(\vec{x}), \sigma_{\vec{\phi}(\vec{x})}(\vec{x})) \quad \rightarrow \quad z^{(l)} = \mu_{\vec{\phi}(\vec{x})}(\vec{x}) + \sigma_{\vec{\phi}(\vec{x})}(\vec{x}) \epsilon^{(l)}, \quad \epsilon^{(l)} \sim \mathcal{N}(0,1)
\end{equation}

(where $\mu$ and $\sigma$ are the outputs of the encoder, so we kind of have a non-linear
factor analysis model) then we can rewrite

\begin{equation}
    E_{z \, \sim \, q_{\vec{\phi}(\vec{x})}(z)}[h(z)] = E_{\epsilon \, \sim \, \mathcal{N}(0,1)} \left[ h(g_{\vec{\phi}(\vec{x})}(\epsilon, \vec{x})) \right], \quad \text{some general function } h
\end{equation}

so $\partial_\vec{\phi}$ can be drawn into the expectation / the sampling in the empirical estimation
does not depend on $\vec{\phi}$.
}

\bluebox{\textbf{Generative process:} For using the VAE as a generative model the most flexible approach is to train
another model to obtain a learned prior $p(\vec{z})$ (as for instance in the original DALLE
\footnote{
\enquote{The overall procedure can be viewed as maximizing the evidence lower bound on the joint
likelihood of the model distribution over images, captions and tokens for the encoded RGB image.} 
A dVAE learns the token distribution $q_\vec{\phi}$ in latent space given the RGB image, and
the distribution over images given the latent image tokens. A transformer learns
the joint distribution over text and latent image tokens, so the prior 
used for generative modeling.
}).}

So we do not directly sample from $q_{\vec{\phi}(\vec{x})}(\vec{z})$, but from a surrogate model, where the distribution does not
depend on the parameters $\vec{\phi}$ but we still ensure that the sampled $\vec{z}$ follows our variational density. This
allows us to put the derivative inside the expectation.

\pagebreak