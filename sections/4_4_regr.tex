\section{Regression}
\thispagestyle{plain}

\textcolor{blue1}{Aim of regression:} In Regression analysis, based on a sample,
we are trying to find the relationship between a dependent (continuous) variable
$y$ (\textit{outcome, response}) and one or more independent variables collected 
in the vector $\vec{x}$ (\textit{features, predictors, regressors, explanatory variables}).

\begin{equation}
    \begin{gathered}
        D = \{ y_i, \vec{x}_i \}_{i=1}^n, \quad \text{model } E[y_i|\vec{x}_i] = f_\vec{\theta}(\vec{x}_i) \\
        \text{parameters } \vec{\theta}, \quad \text{continuous variable } y
    \end{gathered}
\end{equation}

Possible aims in regression analysis are
\begin{itemize}
    \item based on the estimated model parameters from a sample, predict the outcome for new observations
    \item understand the relationship between the outcome and the predictors
\end{itemize}

\pinkbox{\textbf{Primer on machine learning:} In Machine Learning (ML) we want to find meaningful
patterns in data. We can roughly divide ML into
\begin{itemize}
    \item \textbf{Supervised learning:} We have input-output pairs (aka labeled data, with the outcome variable being the labeling), with the basic aim being labeling (aka making a prediction of the output for) new (unseen) data
    \begin{itemize}
        \item \textbf{Regression:} here the continuous $y_i$ is the outcome variable, our labeling (e.g. given environmental factors $\vec{x}$ we want to predict the temperature)
        \item \textbf{Classification:} here the $y_i$ is a categorical (nominal) variable, and we model category probabilities, e.g. for $\vec{x}$ being some representation of an image
        we want to assign it to one of some categories (e.g. elephant, cow, ...)
    \end{itemize}
    \item \textbf{Unsupervised learning:} We have only input data, and we want to find patterns in it
    \begin{itemize}
        \item \textbf{Clustering:} We want to find groups of similar observations (e.g. K-means, agglomerative clustering, ...)
        \item \textbf{Dimensionality reduction:} We want to find a lower-dimensional representation of the data
    \end{itemize}
    \item \textbf{Reinforcement learning:} We want to find a policy for an agent to maximize some reward
\end{itemize}}

\note{While regression and classification differ in that the outcome variable is continuous in the former and categorical in the latter, they are also connected.
For instance in the Generalized Linear Model (GLM) framework, we can also model class probabilities, e.g. for logistic regression, which really
is a classification model.}

\subsection{Multiple Linear Regression | the standard}
\subsubsection{Model setup | what we model how under which assumptions}
\textcolor{green1}{Goals:} Consider we have measured
\begin{equation}
    \begin{gathered}
        \text{features } \vec{x}_i = \begin{pmatrix} x_{i1} \\ x_{i2} \\ \vdots \\ x_{ip} \end{pmatrix}, \text{e.g. trace gas concentrations} \\
        \text{outcome } y_i, \text{e.g. temperature}
    \end{gathered}
\end{equation}
and we want to
\begin{itemize}
    \item predict the temperature at given trace gas concentrations
    \item find which trace gas concentrations are important for the temperature (important explanatory variables)
\end{itemize}

\textcolor{blue1}{Assumed underlying model:} We assume that the data $D = \{ y_i, \vec{x}_i \}_{i=1}^n$ is generated by the model (data modelling culture)
\begin{equation}
    \begin{gathered}    
        y(\vec{x}) = \vec{\beta}^* \cdot \vec{x} + \epsilon(x), \quad \text{true parameter } \vec{\beta}^* = \left( \begin{array}{c} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{array} \right) \\
        \text{augmented independent variables} \left( \begin{array}{c} 1 \\ x_1 \\ \vdots \\ x_p \end{array} \right)
    \end{gathered}
\end{equation}
The random variables are $y$ and $\epsilon$. The $\beta_i$ are non-random but unobservable.

\note{The model is linear in the parameters but not necessarily in the original features - we can e.g.
get to polynomial regression, by expanding the feature-vector by powers of the original features (\textit{feature engineering}).}

\subsubsubsection{Assumptions}

We make the general assumption that
\begin{enumerate}
    \item $\vec{x}$ is deterministic and exact, there is no error in the explanatory variables \textcolor{red1}{(which is not true for experimental data)}\footnote{If the experimental errors in $\vec{x}$ are to be considered, resort to orthogonal distance regression.}
\end{enumerate}

When we add the \textbf{Gauss-Markov assumptions}
\begin{enumerate}
    \setcounter{enumi}{1}
    \item $E[\epsilon(\vec{x})] = 0$, so a linear model is applicable (the error is not systematically biased)
    \item $Var[\epsilon(\vec{x})] = \sigma^2 = \text{const.} < \infty$ independent of $\vec{x}$ (homoscedasticity), otherwise
    high variance regions would induce instability (see figure \ref{fig:homo_hetero})
    \item $Cov[\epsilon(\vec{x}_i), \epsilon(\vec{x}_j)] = 0$ for $i \neq j$, so the errors are uncorrelated
\end{enumerate}
then the best \textbf{linear unbiased estimator} (BLUE) of $\vec{\beta}^*$ is the \textbf{Ordinary Least Squares} (OLS) estimator $\hat{\vec{\beta}}$ (by the \textbf{Gauss-Markov theorem}).
Best in the sense that it has the \textcolor{green1}{lowest sampling variance} among all linear unbiased estimators.

\begin{figure}[!htb]
 \centering
 \includesvg[width=0.7\textwidth]{figures/homo_hetero.svg}\hfill
 \caption{homoscedasticity vs heteroscedasticity}
 \label{fig:homo_hetero}
\end{figure}

Finally, if we additionally assume

\begin{enumerate}
    \setcounter{enumi}{5}
    \item $\epsilon(\vec{x})$ to be independent and identically (i.i.d.) normally distributed
\end{enumerate}

then the sampling distribution of the OLS (ordinary least squares) estimate $\vec{\hat{\beta}}$ will also be normal (see also later).

\greenbox{Generally OLS linear regression is an easy, robust benchmark for more complex models.
In situations with little data a bias in form of a regularization (we can also formulate as a prior) can help.}

\subsubsubsection{Matrix notation}
For the whole sample $\{ \vec{x}_i,y_i \}_{i=1}^N$ we can write
\begin{equation}
 \begin{gathered}
    \vec{y}=\left(\begin{array}{c}
        y_1 \\
        \vdots \\
        y_i \\
        \vdots \\
        y_N
        \end{array}\right), \quad \mat{X}=\left[\begin{array}{cccc}
        1 & x_{11} & \cdots & x_{1 p} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{N 1} & \cdots & x_{N p}
    \end{array}\right] \in \mathbb{R}^{N \times p} \\
    N \text{ observations, explanatory variable with } p \text{ features} \\
    \mat{X} \text{ with 1 column to allow for constant offset } \beta_0
 \end{gathered}
\end{equation}

so we can write the model as

\begin{equation}
    \vec{\hat{y}}=\mat{X} \vec{\hat{\beta}}
\end{equation}

\subsubsection{Two views on finding an estimator for $\vec{\beta}^*$}
Consider the true model to be
\begin{equation}
    \vec{y}=\mat{X} \vec{\beta}^*+\vec{\epsilon}, \quad \vec{\epsilon} \sim \mathcal{N}\left(0, \sigma^2 \mat{1}_{N \times N}\right), \quad \text { some true parameters } \vec{\beta}^{\star}, \quad \epsilon_i \text { i.i.d. }
\end{equation}
$\mat{X}$ is deterministic and $y_i$ are distributed normally around a modeled mean.
The basic intuitions for finding an estimator $\hat{\vec{\beta}}$ for $\vec{\beta}^*$ are
\begin{itemize}
    \item \textbf{Least squares:} Imagine springs between data points and model (along $y$), minimizing the overall energy
    \item \textbf{Maximum likelihood:} Imagine the data points as the result of a stochastic process, we want to find the model parameters that make the data most likely
\end{itemize}
both lead to the same estimator $\hat{\vec{\beta}}$. The two intuitions are illustrated in 
figure \ref{fig:two_views_lin}.

\begin{figure}[!htb]
 \centering
 \includesvg[width=0.8\textwidth]{figures/two_views_lin.svg}\hfill
 \caption{Two views on finding an estimator for $\vec{\beta}^*$}
 \label{fig:two_views_lin}
\end{figure}

\subsubsection{Finding the model parameters minimizing the sum of squared errors}
We can calculate the sum of squares residual (a scalar) as

\begin{equation}
    \operatorname{SSQ}(\vec{\beta})=\|\vec{Y}-\mat{X} \vec{\beta}\|_2^2=(\vec{Y}-\mat{X} \vec{\beta})(\vec{Y}-\mat{X} \vec{\beta})^T=\|\vec{\epsilon}\|_2^2
\end{equation}

which is nicely convex, so we can find a unique minimum by

\begin{equation}
    \begin{gathered}
    \frac{\partial \operatorname{SSQ}}{\partial \vec{\beta}}=-2 \mat{X}^T(\vec{Y}-\mat{X} \vec{\beta})_{!}=0 \\
    \rightarrow \mat{X}^T \vec{Y}=\mat{X}^T \mat{X} \vec{\beta} \rightarrow \vec{\hat{\beta}}=\left(\mat{X}^T \mat{X}\right)^{-1} \mat{X}^T \vec{Y} \text { (normal equation) }
    \end{gathered}
\end{equation}

